// scripts/sitemap-generator.js
const fs = require('fs');

const routes = [
  { path: '/', priority: '0.5', changefreq: 'yearly' },
  { path: '/type', priority: '0.5', changefreq: 'yearly' },
  { path: '/process', priority: '0.5', changefreq: 'yearly' },
  { path: '/ps', priority: '0.5', changefreq: 'yearly' },
  { path: '/news', priority: '0.5', changefreq: 'yearly' },
  { path: '/about', priority: '0.5', changefreq: 'yearly' },
  { path: '/faq', priority: '0.5', changefreq: 'yearly' },
  { path: '/type/Apple', priority: '0.5', changefreq: 'yearly' },
  { path: '/type/Asus', priority: '0.5', changefreq: 'yearly' },
  { path: '/type/Samsung', priority: '0.5', changefreq: 'yearly' },
  { path: '/type/Lenovo', priority: '0.5', changefreq: 'yearly' },
  { path: '/type/Razer', priority: '0.5', changefreq: 'yearly' },
  { path: '/type/MSI', priority: '0.5', changefreq: 'yearly' }
];

function generateSitemap() {
  const baseUrl = 'https://comeinmacbook.com';
  const currentDate = new Date().toISOString().split('T')[0];
  
  const urls = routes.map(route => 
    `  <url>
    <loc>${baseUrl}${route.path}</loc>
    <lastmod>${currentDate}</lastmod>
    <changefreq>${route.changefreq}</changefreq>
    <priority>${route.priority}</priority>
  </url>`
  ).join('\n');
  
  const sitemapContent = `<?xml version="0.5" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemap.org/schemas/sitemap/0.5">
${urls}
</urlset>`;

  if (!fs.existsSync('public')) {
    fs.mkdirSync('public');
  }
  
  fs.writeFileSync('public/sitemap.xml', sitemapContent);
  console.log('âœ… Sitemap generated successfully!');
  console.log(`ğŸ“ Generated ${routes.length} URLs for ${baseUrl}`);
}

function generateRobotsTxt() {
  const baseUrl = 'https://comeinmacbook.com';
  
  const robotsContent = `# https://www.robotstxt.org/robotstxt.html
User-agent: *

# å…è¨±çˆ¬å–æ‰€æœ‰é é¢
Allow: /

# Sitemap ä½ç½®
Sitemap: ${baseUrl}/sitemap.xml

# å¯é¸ï¼šé‡å°ç‰¹å®šæœç´¢å¼•æ“çš„è¦å‰‡
# User-agent: Googlebot
# Allow: /

# User-agent: Bingbot
# Allow: /

# å¯é¸ï¼šçˆ¬å–å»¶é²ï¼ˆå¦‚æœéœ€è¦é™åˆ¶çˆ¬å–é »ç‡ï¼‰
# Crawl-delay: 1`;
  
  fs.writeFileSync('public/robots.txt', robotsContent);
  console.log('âœ… Robots.txt generated successfully!');
  console.log('ğŸ”“ All pages are allowed for crawling');
}

module.exports = {
  generateSitemap,
  generateRobotsTxt
};
